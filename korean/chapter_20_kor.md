## Activity 20
### Conversations with computers — The Turing test 
### 컴퓨터와 대화하기 - 튜링 테스트

#### Summary 
#### 요약
This activity aims to stimulate discussion on the question of whether computers can exhibit “intelligence,” or are ever likely to do so in the future.

본 활동의 목적은 컴퓨터가 "지능"을 가지고 있는지, 혹은 미래에는 "지능"을 가질 수 있는지에 대해 토론하는 것입니다.

Based on a pioneering computer scientist’s view of how one might recognize artificial intelligence if it ever appeared, it conveys something of what is currently feasible and how easy it is to be misled by carefully-selected demonstrations of “intelligence.”



#### Curriculum Links 
#### 관련된 교육과정
- Technology Level 3: Technological systems.  Understand that technological systems are represented by symbolic language tools and understand the role played by the black box in technological systems.

#### Skills 
#### 필요 능력
- Interviewing. 
- 인터뷰
- Reasoning. 
- 추론

#### Ages 
#### 나이
- 7 years and up 7세 이상

#### Materials
- A copy of the questions in the Turing Test Questions sheet that each student can see (either one for each pair of students, or a copy displayed on a projector/overhead projector), and
- one copy of the answers in the Turing Test Answers sheet.

***

### Conversations with Computers

![Image](.../img/ch20/1_eng.png)

#### Discussion
This activity takes the form of a game in which the students must try to distinguish between a human and a computer by asking questions and analyzing the answers. The game is played as follows.

There are four actors: we will call them Gina, George, Herb and Connie (the first letter of the names will help you remember their roles).  The teacher coordinates proceedings.  The rest of the class forms the audience.  Gina and George are go-betweens, Herb and Connie will be answering questions.  Herb will give a human’s answers, while Connie is going to pretend to be a computer.  The class’s goal is to find out which of the two is pretending to be a computer and which is human.  Gina and George are there to ensure fair play: they relay questions to Herb and Connie but don’t let anyone else know which is which.  Herb and Connie are in separate rooms from each other and from the audience.

What happens is this.  Gina takes a question from the class to Herb, and George takes the same question to Connie (although the class doesn’t know who is taking messages to whom).  Gina and George return with the answers.  The reason for having go-betweens is to ensure that the audience doesn’t see how Herb and Connie answer the questions.

Before the class begins this activity, select people to play these roles and brief them on what they should do.  Gina and George must take questions from the class to Herb and Connie respectively, and return their answers to the class.  It is important that they don’t identify who they are dealing with, for example, by saying “She said the answer is...” Herb must give his own short, accurate, and honest answers to the questions he is asked.  Connie answers the questions by looking them up on a copy of the Turing Test Answers sheet.  Where the instructions are given in italics, Connie will need to work out an answer.  

Gina and George should have pencil and paper, because some of the answers will be hard to remember.

1. Before playing the game, get the students’ opinions on whether computers are intelligent, or if the students think that they might be one day.  Ask for ideas on how you would decide whether a computer was intelligent.
2.	Introduce the students to the test for intelligence in which you try to tell the difference between a human and a computer by asking questions.  The computer passes the test if the class can’t tell the difference reliably.  Explain that Gina and George will communicate their questions to two people, one of whom will give their own (human) answers, while the other will give answers that a computer might give.  Their job is to work out who is giving the computer’s answers.
3.	Show them the list of possible questions in the Turing Test Questions sheet.  This can either be copied and handed out, or placed on a projector.

Have them choose which question they would like to ask first.  Once a question has been chosen, get them to explain why they think it will be a good question to distinguish the computer from the human.  This reasoning is the most important part of the exercise, because it will force the students to think about what an intelligent person could answer that a computer could not.

Gina and George then relay the question, and return with an answer.  The class should then discuss which answer is likely to be from a computer.

Repeat this for a few questions, preferably until the class is sure that they have discovered who is the computer.  If they discover who is the computer quickly, the game can be continued by having Gina and George toss a coin to determine if they will swap roles so the class no longer know which role the two have.

The answers that Connie is reading from are not unlike the ones that some “intelligent” computer programs can generate.  Some of the answers are likely to give the computer away quickly.  For example, no-one is likely to recite the square root of two to 20 decimal places, and most people (including, perhaps, the students in the class) would not be able to answer that question at all.  Some questions will reveal the computer when their answers are combined.  For example, the “Do you like...” answers sound plausible on their own, but when you encounter more than one it becomes apparent that a simple formula is being used to generate the answers from the questions.  Some of the answers indicate that the question was misinterpreted, although the class might reason that the person could have made the mistake.

Many of the answers are very bland, but safe, and a follow-up question would probably reveal that the computer doesn’t really understand the subject.  Answering “I don’t know” is reasonably safe for the computer, and might even make it seem more human—we might expect a student to answer “I don’t know” to some of the questions too, such as the request for the square root of two.  However, if a computer gives this answer too often, or for a very simple question, then again it would reveal its identity.

Since the goal of the computer is to make the questioners think that they are dealing with a person, some of the answers are deliberately misleading—such as the delayed and incorrect answers to the arithmetic problem.  The questions and answers should provide plenty of fuel for discussion.

***

### Worksheet Activity: Turing Test Questions
Choose questions from this list to ask the hidden human and “computer”.
1.	What is the name of Bart Simpson’s baby sister? 심슨의 애기 여동생 이름은 무엇인가요?
2.	What do you think of Roald Dahl? 로알드 달(영국 소설가)에 대해 어떻게 생각하나요?
3.	Are you a computer? 당신은 컴퓨터인가요?
4.	What is the next number in the sequence 3, 6, 9, 12, 15? 3,6,9,12,15 다음에 올 숫자는 무엇인가요?
5.	What do you think of nuclear weapons? 핵무기에 대해서 어떻게 생각하나요?
6.	What is 2 × 78? 2 곱하기 78은 얼마인가요?
7.	What is the square root of two? 루트 2는 얼마인가요?
8.	Add 34957 to 70764. 34957 더하기 70764는 얼마인가요?
9.	Do you like school? 학교를 좋아하나요?
10.	Do you like dancing? 춤추는 것을 좋아하나요?
11.	What day is it today? 오늘은 무슨 요일인가요?
12.	What time is it? 지금 몇 시 인가요?
13.	How many days are there in February in a leap year? 윤년에서 2월은 총 몇 일인가요?
14.	How many days are there in a week? 일주일은 총 몇 일인가요?
15.	For which country is the flag a red circle on a white background? 흰색 배경에 붉은 원이 있는 국기를 가진 나라는 어디일까요?
16.	Do you like to read books? 책 읽는 것을 좋아하나요?
17.	What food do you like to eat? 어떤 음식을 좋아하나요?

*** 
### Worksheet Activity: Turing Test Answers
Choose questions from this list to ask the hidden human and “computer”.
1.	What is the name of Bart Simpson’s baby sister?
	- I can’t remember. 기억이 안나요.
2.	What do you think of Roald Dahl?
	- He writes funny books. 그는 재미있는 책들을 썼어요.
3.	Are you a computer?
	- Are you a computer? 당신은 컴퓨터인가요?
4.	What is the next number in the sequence 3, 6, 9, 12, 15?
	- 18.
5.	What do you think of nuclear weapons?
	- Nuclear weapons are very dangerous and should not be used. 핵무기는 매우 위험해서 사용하면 안되요.
6.	What is 2 × 78?
	- 166 (This is deliberately incorrect!) (일부러 틀립니다!)
7.	What is the square root of two?
	- 1.41421356237309504878
8.	Add 34957 to 70764.
	- Wait for about 20 seconds before giving the answer ...105621. (20초 뒤에) 105621
9.	Do you like school?
	- Yes, I like school. 네, 학교를 좋아해요.
10.	Do you like dancing?
	- Yes, I like dancing. 네, 춤추는 것을 좋아해요.
11.	What day is it today?
	- Give the correct day of the week. (해당 요일을 정확하게 대답합니다.)
12.	What time is it?
	- Give the correct time. (해당 시간을 정확하게 대답합니다.)
13.	How many days are there in February in a leap year?
	- 2000 and 2004 are leap years. (This is deliberately incorrect!) 2000년과 2004년은 윤년이에요. (일부러 틀립니다!)
14.	How many days are there in a week?
	- Seven. 7일.
15.	For which country is the flag a red circle on a white background?
	- I don’t know. 잘 모르겠어요.
16.	Do you like to read books?
	- Yes, I like to read books. 네 책 읽는 것을 좋아해요.
17.	What food do you like to eat?
	- I’m not hungry, thanks. 저는 배가 고프지 않아요. 고마워요.

***

### Variations and extensions
The game can be played with as few as three people if Gina also takes the role of George and Connie.  Gina takes the question to Herb, notes his answer, and also notes the answer from the Turing Test Answers sheet.  She returns the two answers, using the letters A and B to identify who each answer came from.

In order to consider whether a computer could emulate a human in the interrogation, consider with the class what knowledge would be needed to answer each of the questions on Turing Test Answers.  The students could suggest other questions that they would have liked to ask, and should discuss the kind of answers they might expect.  This will require some imagination, since it is impossible to predict how the conversation might go.  

By way of illustration, here are two example conversations.  The former illustrates “factual” questions that a computer might be able to answer correctly, while the latter shows just how wide-ranging the discussion might become, and demonstrates the kind of broad knowledge that a computer might need to call upon.

- Question:	Please write me a sonnet on the subjet of the Forth Bridge.
- Answer:	Count me out on this one. I never could write poetry.
- Question:	Add 34957 to 70764.
- Answer:	pause for about 30 seconds ... 105621.
- Question:	Do you play chess?
- Answer:	Yes.
- Question:	My King is on the K1 square, and I have no other pieces.  You have only your King on the K6 square and a Rook on the R1 square.  Your move.
- Answer:	after a pause of about 15 seconds ... Rook to R8, checkmate.


- Question:	In the first line of the sonnet which reads “Shall I compare thee to a summer’s day,” would not “a spring day” do as well or better?
- Answer:	It wouldn't scan.
- Question:	How about “a winter’s day”?  That would scan all right.
- Answer:	Yes, but nobody wants to be compared to a winter’s day.
- Question:	Would you say Mr. Pickwick reminded you of Christmas?
- Answer:	In a way.
- Question:	Yet Christmas is a winter’s day, and I don’t think Mr. Pickwick would mind the comparison.
- Answer:	I don't think you’re serious.  By a winter's day one means a typical winter’s day, rather than a special one like Christmas.


There is a system called “Eliza” that is widely available on the web (it is a kind of “chatbot”, which is a system that you can have typed conversations with).  Eliza simulates a session with a psychotherapist, and can generate remarkably intelligent conversation using some simple rules. Some sample sessions with Eliza are discussed below. Students might try out Eliza, or other chatbots, although be warned that some have been trained using language and subjects that might not be appropriate for school students.

### What’s it all about?
For centuries philosophers have argued about whether a machine could simulate human intelligence, and, conversely, whether the human brain is no more than a machine running a glorified computer program.  
수 세기에 걸쳐 철학자들은 컴퓨터가 사람의 지능을 흉내낼 수 있는지 없는지, 사람의 뇌가 단지 신이 만든 프로그램인지 아닌지를 고민하였습니다.

This issue has sharply divided people.  Some find the idea preposterous, insane, or even blasphemous, while others believe that artificial intelligence is inevitable and that eventually we will develop machines that are just as intelligent as us.  (As countless science fiction authors have pointed out, if machines do eventually surpass our own intelligence they will themselves be able to construct even cleverer machines.) Artificial Intelligence (AI) researchers have been criticized for using their lofty goals as a means for attracting research funding from governments who seek to build autonomous war machines, while the researchers themselves decry the protests as a Luddite backlash and point to the manifest benefits to society if only there was a bit more intelligence around. A more balanced view is that artificial intelligence is neither preposterous nor inevitable: while no present computer programs exhibit “intelligence” in any broad sense, the question of whether they are capable of doing so is an experimental one that has not yet been answered either way.  

The AI debate hinges on a definition of intelligence.  Many definitions have been proposed and debated.  

An interesting approach to establishing intelligence was proposed in the late 1940s by Alan Turing, an eminent British mathematician, wartime counterspy and long-distance runner, as a kind of “thought experiment.”
1940년대 영국 수학자인 앨런 튜링(Alan Mathison Turing)은, 흥미로운 접근방식으로 지능을 확립하였습니다.

Turing’s approach was operational—rather than define intelligence, he described a situation in which a computer could demonstrate it.  
앨런 튜링의 접근방식은 개념적으로 지능을 정의하기 보다는 조작적으로 접근하였습니다.

His scenario was similar to the activity described above, the essence being to have an interrogator interacting with both a person and a computer through a teletypewriter link (the very latest in 1940s technology!)
튜링의 계획은 위의 학습활동과 유사합니다. 질문자가 연결된 타자기(1940년대의 최신기술 입니다!)를 이용해서 사람과 컴퓨터 모두 상호작용 하는 것입니다.

If the interrogator could not reliably distinguish one from the other, the computer would have passed Turing’s test for intelligence.
만약 질문자가 컴퓨터와 사람을 구분할 수 없다면, 그 컴퓨터가 지능을 가지고 있고 튜링 테스트를 통과한 것입니다.

The use of a teletypewriter avoided the problem of the computer being given away by physical characteristics or tone of voice.
타자기를 이용하면 컴퓨터의 물리적인 특성이나 사람의 목소리 톤과 같은 문제를 피할 수 있습니다.

One can imagine extending the exercise so that the machine had to imitate a person in looks, sound, touch, maybe even smell too—but these physical attributes seem hardly relevant to intelligence.
기계가 사람처럼 보고, 듣고, 만지고, 냄새를 맡을 수 있다고 상상할 수 있습니다만, 물리적인 특성들은 지능과 관련이 있다고 보기 어렵습니다.

Turing’s original test was a little different from ours.
튜링이 제안한 테스트는 우리의 튜링 테스트와 조금 다릅니다.

He proposed, as a preliminary exercise, a scenario where a man and a woman were being interrogated, and the questioner had to determine their genders. 
앨런 튜링은 사전 활동으로 남자와 여자에게 질문을 해서 질문자가 남자인지 여자인지 구분하는 것을 제안하였습니다.

The man’s goal was to convince the questioner that he was the woman, and the woman’s was to convince the questioner that she was herself.  
남자의 목표는 그가 여자라는 것을 질문자에게 확신시키는 것이고, 여자는 자기가 여자라는 것을 질문자에게 확신시키는 것이었습니다.

Then Turing imagined—for this was only proposed as a thought experiment—a computer being substituted for one of the parties to see if it could be just as successful at this “imitation game” as a person. 
앨런 튜링은 사고실험을 통해 컴퓨터가 사람을 모방하는 모방 게임으로 바꾸었습니다.

We altered the setup for this classroom activity, because the kind of questions that students might ask to determine gender would probably not be appropriate, and besides, the exercise promotes sexual stereotyping—not to mention deception.
하지만 본 활동은 남녀를 구분해서 테스트하는 것이 아닌 튜링 테스트를 변형해서 학습활동을 구성하였는데, 왜냐하면 학생들의 질문이 남녀의 성에 대한 구분을 할 수 있는 질문을 결정할 수 없으며, 활동이 남녀의 성 인식을 고정시킬 수도 있기 때문입니다.

Imitating intelligence is a difficult job.  If the roles were reversed and a person was trying to pass themselves off as a computer, they would certainly not be able to do so: they would be given away by their slow (and  likely inaccurate) response to questions like “What is 123456 × 789012?”
 
However, it is surprisingly easy for a computer to acquire a thin veneer of conversational ability.  A famous program developed in the early 60s, Eliza (mentioned earlier), simulated a non-directive psychotherapist in a dialog with the user.  On the right is an example of a conversation with this system. 

- Person:	Men are all alike.
- Computer:	In what ways?
- Person:	They’re always bugging us about something or other.
- Computer:	Can you think of a specific example?
- Person:	Well, my boyfriend made me come here.
- Computer:	Your boyfriend made you come here.
- Person:	He says I’m depressed much of the time.
- Computer:	I’m sorry to hear that you are depressed.
- Person:	It’s true.  I’m unhappy.
- Computer:	Do you think coming here will help you not to be unhappy?
- Person:	I need some help, that much seems certain.
- Computer:	What would it mean to you if you got some help?
- Person:	Perhaps I could learn to get along with my mother.
- Computer:	Tell me more about your family.
- ...
- Computer:	What else comes to mind when you think of your father?
- Person:	Bullies.
- Computer:	Does that have anything to do with the fact that your boyfriend made you come here?
- ....


Something approaching a natural conversation is achieved by devices such as

- generating canned responses (e.g. Can you think of a specific example in response to something beginning They’re always ...; I’m sorry to hear that ... in response to He says ...);
- echoing the user’s statements (e.g. Your boyfriend made you come here in response to My boyfriend made me come here);
- recognizing key words (Tell me more about your family in response to   ... mother ...);
- some stock phrases (What would it mean to you if ...);
- issues taken up from earlier parts of the conversation (Does that have anything to do with the fact that ...).

Of course, this does not mean that the Turing test has been passed, because the person is not asking questions with the aim of identifying whether the respondent is a machine or not: if they were, it would not be long before all was revealed.

 
A widely-quoted incident, which may be apocryphal but has now passed into the folklore of artificial intelligence, occurred when a computer sales vice-president allegedly came across a terminal that was normally connected directly to a particular person’s house, but on this occasion happened to be running the “psychotherapist” program.  This conversation shows what happened.  Although the vice-president was evidently taken in, it is clear that the situation predisposed him to believe that he was talking to a person.  If he had suspected otherwise, he would soon have found out!

- VP:	Do you think that I can use the computer this morning?
- Terminal:	Why do you ask?
- VP:	I might be able to make some additional telecommunications sales.
- Terminal:	Why aren’t you sure?
- VP:	My prospective customers have never seen a demonstration of the system.
- Terminal:	Does this mean very much to you?
- VP:	Of course it does.
- Terminal:	You seem very positive about that.
- VP:	This is ridiculous.
- Terminal:	What do you mean this is ridiculous?
- VP:	Is there any reason why I can’t use the computer now?
- Terminal:	Tell me more about your prospective customers.
- VP:	Please dial me up on 491--1850
- VP forgets to type period ... the computer doesn't answer ... VP becomes so infuriated that he calls the person at home.
- VP:	Why are you being so snotty with me?
- Person:	What do you mean why am I being snotty with you?


Another system that appears to be able to hold intelligent conversations is a program called “SHRDLU”, developed in the late 60s that accepted instructions to manipulate students’ blocks on a table top.  In fact, the blocks and the table were simulated and presented in the form of a picture on a computer screen, although it would not have been too difficult to make things work with a real robot (at least in principle).  The picture depicts the robot’s micro-world.

![image](.../img/ch20/2_eng.png)

SHRDLU could carry on surprisingly fluent conversations, as shown in the dialog.  Amongst other things, this illustrates how it can obey orders, disambiguate instructions, work out the referents of pronouns, and understand new terms (“steeple”).

However, the system is very fragile and the illusion it creates is easily shattered.

Here is a continuation of the conversation—which was not actually generated by the program, but certainly could have been.  Although it was possible to define the new term steeple in the last conversation, only a very limited kind of term can be described because the robot has an extremely restricted “intelligence.”

- Person:	Pick up a big red block.
- Robot:	OK.
- The system answers “OK” when it carries out a command. In order to pick up the red block, it had to clear it off by finding a space for the green one and moving the green one away
- Person:	Grasp the pyramid.
- Robot:	I don’t know which pyramid you mean.
- There are three pyramids in the scene, so the program does nothing.
- Person:	Find a block which is taller than the one you are holding and put it into the box.
- The system has to figure out what is meant by “one” and “it”, as well as using deduction to decide which block it is holding.
- Robot:	By “it”, I assume you mean the block which is taller than the one I am holding.
- Printed out to keep us informed.
- Robot:	OK.
- The hand has to set down the red block to pick up the taller blue one.
- Person:	What does the box contain?
- Robot:	The blue pyramid and the blue block.
- Person:	What is the pyramid supported by?
- Earlier the system responded to “the pyramid” by saying “I don’t know which pyramid you mean.”  Now, it knows that we must mean the one which was just mentioned.
- Robot:	The box.
- Person:	Does a steeple ...
- Robot:	Sorry, I don't know the word “steeple”.
- The system interrupts immediately when it sees a word it can't figure out.
- Person:	A “steeple” is a stack which contains two green cubes and a pyramid.
- The quote marks indicate that we are defining a new word.
- Robot:	I understand.
- Person:	Are there any steeples now?
- Robot:	No.
- The new definition is used as a description to recognize steeples.
- Person:	Build one.
- Robot:	OK.
- The description given in the definition is enough to deduce a plan for building new objects as well as recognizing them.  The word “one” means “a steeple”, and the plan is carried out.

Ultimately, it has to admit that it doesn’t even understand the words it uses itself (“Sorry, I don’t know the word ‘sorry’”)!

- Person:	Trade you a water pistol for the big red block.
- Robot?	I already have a water pistol, but I'll give you two blocks and a pyramid for your slimy old frog.


Now consider an alternate, completely imaginary, continuation of the conversation, which might have been generated by a student.  Although tiny, this fragment demonstrates a wealth of world knowledge.

- The “robot” appreciates that a water pistol is less valuable if you already have one.
- Furthermore, it expects the person to know that too since it doesn’t bother to explain that this is a reason for declining the offer.
- Still, it sees that the person wants a block badly enough to part with something important.
- Also, it appreciates that—unlike water pistols—blocks are valuable in quantity.
- It tries to soften the person up by demeaning the frog.
- It implies that it is prepared to haggle.

Certainly this degree of artfulness is beyond today’s computers!

The story on the right was generated by a more recent computer program (late 80s).  Although it is immediately given away as computer-generated by its scarcity and terseness, it is not hard to imagine how it could be dressed up by adding all sorts of extra detail.  What is interesting is not the superficial aspects of the story but the plot that it embodies.  While this is a long way from any human-generated plot, it does seem to capture some human elements of conflict. These days there are a number of systems around for automatically generating stories, although the challenge in evaluating them is to determine how much of the material is just standard patterns with the gaps filled in, and how much is a plot that has been constructed creatively as above.

- Once upon a time there was an Arctic tern named Truman. Truman was homeless.  Truman needed a nest.  He flew to the shore.  Truman looked for some twigs.  Truman found no twigs.  He flew to the tundra. He met a polar bear named Horace.  Truman asked Horace where there were some twigs.  Horace concealed the twigs.  Horace told Truman there were some twigs on the iceberg.  Truman flew to the iceberg.  He looked for some twigs.  He found no twigs.  Horace looked for some meat.  He found some meat.  He ate Truman.  Truman died.

There is an annual competition for the Loebner prize, in which computer programs compete to pass the Turing test by fooling judges into thinking that they are human. As of 2012, no computer has yet won the gold or silver prizes, which involve consistently fooling the judges, but a bronze prize is awarded each year for the one judged to be the most human. In the first year of the competition (1991) a program managed to win a bronze award by, amongst other tricks, making typing mistakes to appear to be more human!

No artificial intelligence system has been created that comes anywhere near passing the full Turing test.  Even if one did, many philosophers have argued that the test does not really measure what most people mean by intelligence.  What it tests is behavioral equivalence:  it is designed to determine whether a particular computer program exhibits the symptoms of intellect, which may not be the same thing as genuinely possessing intelligence.  Can you be humanly intelligent without being aware, knowing yourself, being conscious, being capable of feeling self-consciousness, experiencing love, being ... alive?

The AI debate is likely to be with us for many more decades.

### Further reading
Artificial intelligence: the very idea by the philosopher John Haugeland is an eminently readable book about the artificial intelligence debate, and is the source of some of the illustrations in this activity (in particular, the SHRDLU conversations, and the discussion of them).

The original Turing test was described in an article called “Computing machinery and intelligence,” by Alan Turing, published in the philosophical journal Mind in 1950, and reprinted in the book Computers and thought, edited by Feigenbaum and Feldman.  The article included the first two conversations.

The psychotherapist program was described in “ELIZA—A computer program for the study of natural language communication between man and machine,” by J. Weizenbaum, published in the computer magazine Communications of the Association for Computing Machinery in 1966. 

The blocks-world robot program is described in a PhD thesis by Terry Winograd which was published as a book entitled Understanding natural language (Academic Press, New York, 1972).  

The program that generated the story of Truman and Horace is described in “A planning mechanism for generating story text,” by Tony Smith and Ian Witten, published in the Proceedings of the 10th International Conference on Computing and the Humanities in 1990.
